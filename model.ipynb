{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, Dropout\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"validation.csv\")\n",
    "char_to_pred = json.load(open(\"data/character_to_prediction_index.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_id    target\n",
       "0            1  bathroom\n",
       "1            1  bathroom\n",
       "2            1  bathroom\n",
       "3            1  bathroom\n",
       "4            1  bathroom"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Train data--------------------\n",
      "Cantidad de filas : 4925\n",
      "Frases unicas : ['bathroom' 'cat' 'dog' 'eat food' 'father' 'fine' 'finish' 'go to'\n",
      " 'hello' 'help' 'learn' 'like' 'me' 'milk' 'more' 'mother' 'no' 'please'\n",
      " 'repeat' 'see you later' 'sign' 'thank you' 'want' 'what' 'yes']\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------Train data--------------------\")\n",
    "print(f\"Cantidad de filas : {train_data.shape[0]}\")\n",
    "print(f\"Frases unicas : {train_data.target.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Validation data--------------------\n",
      "Cantidad de filas : 356\n",
      "Frases unicas : ['bathroom' 'cat' 'dog' 'eat food' 'father' 'fine' 'finish' 'go to'\n",
      " 'hello' 'help' 'learn' 'like' 'me' 'milk' 'more' 'mother' 'no' 'please'\n",
      " 'repeat' 'see you later' 'sign' 'thank you' 'want' 'what' 'yes']\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------Validation data--------------------\")\n",
    "print(f\"Cantidad de filas : {test_data.shape[0]}\")\n",
    "print(f\"Frases unicas : {test_data.target.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (list(train_data.target.unique()) != list(test_data.target.unique())):\n",
    "    raise ValueError(\"Error between target and train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4925.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178.241827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>105.238701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>359.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sequence_id\n",
       "count  4925.000000\n",
       "mean    178.241827\n",
       "std     105.238701\n",
       "min       1.000000\n",
       "25%      84.000000\n",
       "50%     185.000000\n",
       "75%     267.000000\n",
       "max     359.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance\n",
    "* Ref: https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/#:~:text=The%20Levenshtein%20distance%20is%20a,transform%20one%20word%20into%20another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDistances(distances, token1Length, token2Length):\n",
    "    for t1 in range(token1Length + 1):\n",
    "        for t2 in range(token2Length + 1):\n",
    "            print(int(distances[t1][t2]), end=\" \")\n",
    "        print()\n",
    "\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    printDistances(distances, len(token1), len(token2))\n",
    "    return distances[len(token1)][len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# levenshteinDistanceDP(\"kelm\", \"hello\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento del Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo se obtienen 20 indices de las coordenadas x y y, ya que son las unicas que han sido altamente entrenadas del modelo mediapipe de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_needed_cols():\n",
    "    cols = []\n",
    "\n",
    "    for i in range(21):\n",
    "        cols.append(f'x_right_hand_{i}')\n",
    "        cols.append(f'y_right_hand_{i}')\n",
    "        cols.append(f'x_left_hand_{i}')\n",
    "        cols.append(f'y_left_hand_{i}')\n",
    "    \n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df_test = df[df['sequence_id'].isin(test_data['sequence_id'])]\n",
    "df_train = df[df['sequence_id'].isin(train_data['sequence_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>target</th>\n",
       "      <th>x_right_hand_0</th>\n",
       "      <th>x_right_hand_1</th>\n",
       "      <th>x_right_hand_10</th>\n",
       "      <th>x_right_hand_11</th>\n",
       "      <th>x_right_hand_12</th>\n",
       "      <th>x_right_hand_13</th>\n",
       "      <th>x_right_hand_14</th>\n",
       "      <th>x_right_hand_15</th>\n",
       "      <th>...</th>\n",
       "      <th>y_left_hand_19</th>\n",
       "      <th>y_left_hand_2</th>\n",
       "      <th>y_left_hand_20</th>\n",
       "      <th>y_left_hand_3</th>\n",
       "      <th>y_left_hand_4</th>\n",
       "      <th>y_left_hand_5</th>\n",
       "      <th>y_left_hand_6</th>\n",
       "      <th>y_left_hand_7</th>\n",
       "      <th>y_left_hand_8</th>\n",
       "      <th>y_left_hand_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.292266</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>0.293812</td>\n",
       "      <td>0.295057</td>\n",
       "      <td>0.267202</td>\n",
       "      <td>0.275095</td>\n",
       "      <td>0.279875</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.283851</td>\n",
       "      <td>0.270306</td>\n",
       "      <td>0.277417</td>\n",
       "      <td>0.281006</td>\n",
       "      <td>0.252090</td>\n",
       "      <td>0.258523</td>\n",
       "      <td>0.266719</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>0.259433</td>\n",
       "      <td>0.278479</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.263265</td>\n",
       "      <td>0.270317</td>\n",
       "      <td>0.239482</td>\n",
       "      <td>0.243582</td>\n",
       "      <td>0.254479</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.274468</td>\n",
       "      <td>0.239758</td>\n",
       "      <td>0.253949</td>\n",
       "      <td>0.263418</td>\n",
       "      <td>0.230673</td>\n",
       "      <td>0.232797</td>\n",
       "      <td>0.246966</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bathroom</td>\n",
       "      <td>0.268236</td>\n",
       "      <td>0.273854</td>\n",
       "      <td>0.231170</td>\n",
       "      <td>0.247891</td>\n",
       "      <td>0.259833</td>\n",
       "      <td>0.226620</td>\n",
       "      <td>0.228386</td>\n",
       "      <td>0.245820</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_id    target  x_right_hand_0  x_right_hand_1  x_right_hand_10  \\\n",
       "0            1  bathroom        0.265100        0.292266         0.289448   \n",
       "1            1  bathroom        0.259819        0.283851         0.270306   \n",
       "2            1  bathroom        0.259433        0.278479         0.252867   \n",
       "3            1  bathroom        0.262010        0.274468         0.239758   \n",
       "4            1  bathroom        0.268236        0.273854         0.231170   \n",
       "\n",
       "   x_right_hand_11  x_right_hand_12  x_right_hand_13  x_right_hand_14  \\\n",
       "0         0.293812         0.295057         0.267202         0.275095   \n",
       "1         0.277417         0.281006         0.252090         0.258523   \n",
       "2         0.263265         0.270317         0.239482         0.243582   \n",
       "3         0.253949         0.263418         0.230673         0.232797   \n",
       "4         0.247891         0.259833         0.226620         0.228386   \n",
       "\n",
       "   x_right_hand_15  ...  y_left_hand_19  y_left_hand_2  y_left_hand_20  \\\n",
       "0         0.279875  ...             NaN            NaN             NaN   \n",
       "1         0.266719  ...             NaN            NaN             NaN   \n",
       "2         0.254479  ...             NaN            NaN             NaN   \n",
       "3         0.246966  ...             NaN            NaN             NaN   \n",
       "4         0.245820  ...             NaN            NaN             NaN   \n",
       "\n",
       "   y_left_hand_3  y_left_hand_4  y_left_hand_5  y_left_hand_6  y_left_hand_7  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   y_left_hand_8  y_left_hand_9  \n",
       "0            NaN            NaN  \n",
       "1            NaN            NaN  \n",
       "2            NaN            NaN  \n",
       "3            NaN            NaN  \n",
       "4            NaN            NaN  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5281\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df_test) == len(test_data))\n",
    "print(len(df_train) == len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the lengths of the video sequences\n",
    "video_lengths = df.groupby('sequence_id').size()\n",
    "max_seq_length = video_lengths.max()\n",
    "# max_seq_length = 30\n",
    "# Plot the histogram\n",
    "plt.hist(video_lengths, bins=30)  # Adjust the number of bins as needed\n",
    "plt.xlabel('Video Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Video Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create a one-hot encoder\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_videos(df):\n",
    "    # Create a new DataFrame to store the filled rows\n",
    "    filled_df = pd.DataFrame()\n",
    "    target = []\n",
    "\n",
    "\n",
    "    # Iterate over each group and fill remaining rows with zero\n",
    "    for _, group in df.groupby('sequence_id'):\n",
    "        remaining_rows = max_seq_length - len(group)\n",
    "        if remaining_rows > 0:\n",
    "            zeros_df = pd.DataFrame([[0] * len(group.columns)] * remaining_rows, columns=group.columns)\n",
    "            zeros_df['sequence_id'] = group['sequence_id'].unique()[0]\n",
    "            zeros_df['target'] = group['target'].unique()[0]\n",
    "            group = pd.concat([group, zeros_df])\n",
    "        \n",
    "            filled_df = filled_df.append(group)\n",
    "            target.append(group[\"target\"].unique()[0])\n",
    "        \n",
    "    filled_df.reset_index(drop=True, inplace=True)\n",
    "    filled_df = filled_df.fillna(0)\n",
    "    return filled_df, target\n",
    "\n",
    "def padding_labels(target):\n",
    "    integer_encoded = label_encoder.fit_transform(target)\n",
    "    integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "    # Encode the word \"Hello\"\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)  # sparse=False to get a numpy array as output\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13068 297\n"
     ]
    }
   ],
   "source": [
    "X_train, target = padding_videos(df_train)\n",
    "y_train = padding_labels(target)\n",
    "del X_train[\"sequence_id\"] \n",
    "del X_train[\"target\"] \n",
    "\n",
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_right_hand_0</th>\n",
       "      <th>x_right_hand_1</th>\n",
       "      <th>x_right_hand_10</th>\n",
       "      <th>x_right_hand_11</th>\n",
       "      <th>x_right_hand_12</th>\n",
       "      <th>x_right_hand_13</th>\n",
       "      <th>x_right_hand_14</th>\n",
       "      <th>x_right_hand_15</th>\n",
       "      <th>x_right_hand_16</th>\n",
       "      <th>x_right_hand_17</th>\n",
       "      <th>...</th>\n",
       "      <th>y_left_hand_19</th>\n",
       "      <th>y_left_hand_2</th>\n",
       "      <th>y_left_hand_20</th>\n",
       "      <th>y_left_hand_3</th>\n",
       "      <th>y_left_hand_4</th>\n",
       "      <th>y_left_hand_5</th>\n",
       "      <th>y_left_hand_6</th>\n",
       "      <th>y_left_hand_7</th>\n",
       "      <th>y_left_hand_8</th>\n",
       "      <th>y_left_hand_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.292266</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>0.293812</td>\n",
       "      <td>0.295057</td>\n",
       "      <td>0.267202</td>\n",
       "      <td>0.275095</td>\n",
       "      <td>0.279875</td>\n",
       "      <td>0.280997</td>\n",
       "      <td>0.250408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.283851</td>\n",
       "      <td>0.270306</td>\n",
       "      <td>0.277417</td>\n",
       "      <td>0.281006</td>\n",
       "      <td>0.252090</td>\n",
       "      <td>0.258523</td>\n",
       "      <td>0.266719</td>\n",
       "      <td>0.270370</td>\n",
       "      <td>0.236570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.259433</td>\n",
       "      <td>0.278479</td>\n",
       "      <td>0.252867</td>\n",
       "      <td>0.263265</td>\n",
       "      <td>0.270317</td>\n",
       "      <td>0.239482</td>\n",
       "      <td>0.243582</td>\n",
       "      <td>0.254479</td>\n",
       "      <td>0.260531</td>\n",
       "      <td>0.225768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.274468</td>\n",
       "      <td>0.239758</td>\n",
       "      <td>0.253949</td>\n",
       "      <td>0.263418</td>\n",
       "      <td>0.230673</td>\n",
       "      <td>0.232797</td>\n",
       "      <td>0.246966</td>\n",
       "      <td>0.255380</td>\n",
       "      <td>0.219892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.268236</td>\n",
       "      <td>0.273854</td>\n",
       "      <td>0.231170</td>\n",
       "      <td>0.247891</td>\n",
       "      <td>0.259833</td>\n",
       "      <td>0.226620</td>\n",
       "      <td>0.228386</td>\n",
       "      <td>0.245820</td>\n",
       "      <td>0.257619</td>\n",
       "      <td>0.218765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13063</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13064</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13065</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13066</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13067</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13068 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x_right_hand_0  x_right_hand_1  x_right_hand_10  x_right_hand_11  \\\n",
       "0            0.265100        0.292266         0.289448         0.293812   \n",
       "1            0.259819        0.283851         0.270306         0.277417   \n",
       "2            0.259433        0.278479         0.252867         0.263265   \n",
       "3            0.262010        0.274468         0.239758         0.253949   \n",
       "4            0.268236        0.273854         0.231170         0.247891   \n",
       "...               ...             ...              ...              ...   \n",
       "13063        0.000000        0.000000         0.000000         0.000000   \n",
       "13064        0.000000        0.000000         0.000000         0.000000   \n",
       "13065        0.000000        0.000000         0.000000         0.000000   \n",
       "13066        0.000000        0.000000         0.000000         0.000000   \n",
       "13067        0.000000        0.000000         0.000000         0.000000   \n",
       "\n",
       "       x_right_hand_12  x_right_hand_13  x_right_hand_14  x_right_hand_15  \\\n",
       "0             0.295057         0.267202         0.275095         0.279875   \n",
       "1             0.281006         0.252090         0.258523         0.266719   \n",
       "2             0.270317         0.239482         0.243582         0.254479   \n",
       "3             0.263418         0.230673         0.232797         0.246966   \n",
       "4             0.259833         0.226620         0.228386         0.245820   \n",
       "...                ...              ...              ...              ...   \n",
       "13063         0.000000         0.000000         0.000000         0.000000   \n",
       "13064         0.000000         0.000000         0.000000         0.000000   \n",
       "13065         0.000000         0.000000         0.000000         0.000000   \n",
       "13066         0.000000         0.000000         0.000000         0.000000   \n",
       "13067         0.000000         0.000000         0.000000         0.000000   \n",
       "\n",
       "       x_right_hand_16  x_right_hand_17  ...  y_left_hand_19  y_left_hand_2  \\\n",
       "0             0.280997         0.250408  ...             0.0            0.0   \n",
       "1             0.270370         0.236570  ...             0.0            0.0   \n",
       "2             0.260531         0.225768  ...             0.0            0.0   \n",
       "3             0.255380         0.219892  ...             0.0            0.0   \n",
       "4             0.257619         0.218765  ...             0.0            0.0   \n",
       "...                ...              ...  ...             ...            ...   \n",
       "13063         0.000000         0.000000  ...             0.0            0.0   \n",
       "13064         0.000000         0.000000  ...             0.0            0.0   \n",
       "13065         0.000000         0.000000  ...             0.0            0.0   \n",
       "13066         0.000000         0.000000  ...             0.0            0.0   \n",
       "13067         0.000000         0.000000  ...             0.0            0.0   \n",
       "\n",
       "       y_left_hand_20  y_left_hand_3  y_left_hand_4  y_left_hand_5  \\\n",
       "0                 0.0            0.0            0.0            0.0   \n",
       "1                 0.0            0.0            0.0            0.0   \n",
       "2                 0.0            0.0            0.0            0.0   \n",
       "3                 0.0            0.0            0.0            0.0   \n",
       "4                 0.0            0.0            0.0            0.0   \n",
       "...               ...            ...            ...            ...   \n",
       "13063             0.0            0.0            0.0            0.0   \n",
       "13064             0.0            0.0            0.0            0.0   \n",
       "13065             0.0            0.0            0.0            0.0   \n",
       "13066             0.0            0.0            0.0            0.0   \n",
       "13067             0.0            0.0            0.0            0.0   \n",
       "\n",
       "       y_left_hand_6  y_left_hand_7  y_left_hand_8  y_left_hand_9  \n",
       "0                0.0            0.0            0.0            0.0  \n",
       "1                0.0            0.0            0.0            0.0  \n",
       "2                0.0            0.0            0.0            0.0  \n",
       "3                0.0            0.0            0.0            0.0  \n",
       "4                0.0            0.0            0.0            0.0  \n",
       "...              ...            ...            ...            ...  \n",
       "13063            0.0            0.0            0.0            0.0  \n",
       "13064            0.0            0.0            0.0            0.0  \n",
       "13065            0.0            0.0            0.0            0.0  \n",
       "13066            0.0            0.0            0.0            0.0  \n",
       "13067            0.0            0.0            0.0            0.0  \n",
       "\n",
       "[13068 rows x 84 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100 25\n"
     ]
    }
   ],
   "source": [
    "X_test, target = padding_videos(df_test)\n",
    "y_test = padding_labels(target)\n",
    "del X_test[\"sequence_id\"] \n",
    "del X_test[\"target\"] \n",
    "\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train) + len(y_test) == len(df[\"sequence_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_X(X):\n",
    "    # Define the number of rows to be flattened\n",
    "    rows_to_flatten = max_seq_length\n",
    "\n",
    "    data_array = X.to_numpy()\n",
    "\n",
    "    # Get the number of resulting rows in the output array\n",
    "    resulting_rows = data_array.shape[0] // rows_to_flatten\n",
    "\n",
    "    # Reshape the array to have (resulting_rows, rows_to_flatten, 80) shape\n",
    "    reshaped_array = data_array[:resulting_rows * rows_to_flatten].reshape(resulting_rows, rows_to_flatten, -1)\n",
    "\n",
    "    # Flatten the reshaped array along the second axis (axis=1) to get (resulting_rows, 13600) shape\n",
    "    flattened_array = reshaped_array.reshape(resulting_rows, -1)\n",
    "\n",
    "    return flattened_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_size =  num_classes * num_timesteps * num_features\n",
    "# actual_size = X.iloc[:, :num_features].values.size\n",
    "# if expected_size != actual_size:\n",
    "#     raise ValueError(\"The total number of elements in the DataFrame does not match the expected size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = int(len(X_test)/max_seq_length)\n",
    "num_features = len(get_needed_cols())\n",
    "num_classes = len(y_test[1])\n",
    "\n",
    "X_test = X_test.values.reshape(num_samples, max_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_train = int(len(X_train)/max_seq_length)\n",
    "num_features_train = len(get_needed_cols())\n",
    "num_classes_train = len(y_train[1])\n",
    "\n",
    "X_train = X_train.values.reshape(num_samples_train, max_seq_length, num_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = flat_X(X_train)\n",
    "# X_test = flat_X(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (297, 44, 84) (297, 25)\n",
      "Test: (25, 44, 84) (25, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "La entrada son las coordenadas de las manos. Cada video cuenta con n cantidad de filas, 84 columnas (21 columnas por cada coordenada y por ambas manos).\n",
    "La salida es la frase. La frase se representa por un entero que da el one hot encoder.\n",
    "\n",
    "Se usa convoluciones para resaltar las caracteristicas en la entrada. Debido a que la entrada son coordenadas normalizadas de un video, se supone que funciona igual que si la entrada fuera una imagen. Estas redes extraen caracteristicas de forma automatica para clasificar objetos luego. Al buscar patrones, se espera que pueda predecir un video que ya ha sido entrenado previamente.\n",
    "\n",
    "Se reduce el tamaño de la entrada haciendo uso de max pooling y flatten.\n",
    "\n",
    "Se hace uso de Dense para conectar entradas con salidas.\n",
    "\n",
    "Se hace uso de Dropout para evitar el sobreajuste.\n",
    "\n",
    "Relu elimina negativos. \n",
    "Sigmoid nos ayuda a obtener la probabilidad de que un ejemplo pertenezca a la clase positiva.\n",
    "Softmax hace clasificacion multiclase (en nuestro caso las palabras a predecir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(max_seq_length, num_features_train)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(num_classes_train, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), strides=(1, 1), input_shape=(max_seq_length, num_features_train, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(25, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes_train, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.3262 - accuracy: 0.0471\n",
      "Epoch 2/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.1244 - accuracy: 0.1111\n",
      "Epoch 3/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.0690 - accuracy: 0.0943\n",
      "Epoch 4/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 3.1010 - accuracy: 0.0909\n",
      "Epoch 5/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.0317 - accuracy: 0.1178\n",
      "Epoch 6/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.0024 - accuracy: 0.1212\n",
      "Epoch 7/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.9635 - accuracy: 0.1212\n",
      "Epoch 8/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 2.9607 - accuracy: 0.1246\n",
      "Epoch 9/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.9198 - accuracy: 0.1246\n",
      "Epoch 10/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 2.8730 - accuracy: 0.1549\n",
      "Epoch 11/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8571 - accuracy: 0.1313\n",
      "Epoch 12/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8612 - accuracy: 0.1347\n",
      "Epoch 13/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8423 - accuracy: 0.1448\n",
      "Epoch 14/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8200 - accuracy: 0.1650\n",
      "Epoch 15/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 2.7826 - accuracy: 0.1785\n",
      "Epoch 16/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.7948 - accuracy: 0.1684\n",
      "Epoch 17/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.7902 - accuracy: 0.1549\n",
      "Epoch 18/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.7975 - accuracy: 0.1582\n",
      "Epoch 19/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.7557 - accuracy: 0.1650\n",
      "Epoch 20/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.7456 - accuracy: 0.1751\n",
      "Epoch 21/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.6814 - accuracy: 0.1684\n",
      "Epoch 22/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.6933 - accuracy: 0.2088\n",
      "Epoch 23/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.6721 - accuracy: 0.1818\n",
      "Epoch 24/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.6886 - accuracy: 0.1717\n",
      "Epoch 25/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.6344 - accuracy: 0.1886\n",
      "Epoch 26/200\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 2.6203 - accuracy: 0.1684\n",
      "Epoch 27/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.6357 - accuracy: 0.1852\n",
      "Epoch 28/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.5151 - accuracy: 0.2189\n",
      "Epoch 29/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.5893 - accuracy: 0.2189\n",
      "Epoch 30/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.5065 - accuracy: 0.2121\n",
      "Epoch 31/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.5020 - accuracy: 0.2256\n",
      "Epoch 32/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.5114 - accuracy: 0.2189\n",
      "Epoch 33/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 2.5531 - accuracy: 0.2121\n",
      "Epoch 34/200\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 2.4874 - accuracy: 0.2761\n",
      "Epoch 35/200\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 2.4140 - accuracy: 0.2559\n",
      "Epoch 36/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.3460 - accuracy: 0.2997\n",
      "Epoch 37/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.3663 - accuracy: 0.2997\n",
      "Epoch 38/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.3173 - accuracy: 0.3199\n",
      "Epoch 39/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2994 - accuracy: 0.2896\n",
      "Epoch 40/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2594 - accuracy: 0.3434\n",
      "Epoch 41/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2925 - accuracy: 0.3333\n",
      "Epoch 42/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2960 - accuracy: 0.3232\n",
      "Epoch 43/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2854 - accuracy: 0.3131\n",
      "Epoch 44/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.2508 - accuracy: 0.2694\n",
      "Epoch 45/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.1912 - accuracy: 0.3300\n",
      "Epoch 46/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.1499 - accuracy: 0.3636\n",
      "Epoch 47/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.0896 - accuracy: 0.3636\n",
      "Epoch 48/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.1562 - accuracy: 0.3569\n",
      "Epoch 49/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.0938 - accuracy: 0.3872\n",
      "Epoch 50/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.0269 - accuracy: 0.4377\n",
      "Epoch 51/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.0100 - accuracy: 0.4377\n",
      "Epoch 52/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.0180 - accuracy: 0.3737\n",
      "Epoch 53/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 2.0398 - accuracy: 0.3906\n",
      "Epoch 54/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.9635 - accuracy: 0.3973\n",
      "Epoch 55/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.9940 - accuracy: 0.3906\n",
      "Epoch 56/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.9097 - accuracy: 0.4242\n",
      "Epoch 57/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.9542 - accuracy: 0.4007\n",
      "Epoch 58/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.9336 - accuracy: 0.4209\n",
      "Epoch 59/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.9287 - accuracy: 0.4242\n",
      "Epoch 60/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.8639 - accuracy: 0.4343\n",
      "Epoch 61/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.8214 - accuracy: 0.4949\n",
      "Epoch 62/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.7524 - accuracy: 0.5051\n",
      "Epoch 63/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.7584 - accuracy: 0.4983\n",
      "Epoch 64/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.7785 - accuracy: 0.4747\n",
      "Epoch 65/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.7585 - accuracy: 0.4815\n",
      "Epoch 66/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.7269 - accuracy: 0.4781\n",
      "Epoch 67/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.7070 - accuracy: 0.5185\n",
      "Epoch 68/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.6359 - accuracy: 0.5421\n",
      "Epoch 69/200\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 1.7127 - accuracy: 0.5152\n",
      "Epoch 70/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.7407 - accuracy: 0.5219\n",
      "Epoch 71/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.6909 - accuracy: 0.5219\n",
      "Epoch 72/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.6816 - accuracy: 0.5185\n",
      "Epoch 73/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.6479 - accuracy: 0.5253\n",
      "Epoch 74/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.6828 - accuracy: 0.4916\n",
      "Epoch 75/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.6936 - accuracy: 0.4882\n",
      "Epoch 76/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.5912 - accuracy: 0.4983\n",
      "Epoch 77/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.5330 - accuracy: 0.5657\n",
      "Epoch 78/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5440 - accuracy: 0.5926\n",
      "Epoch 79/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5589 - accuracy: 0.5387\n",
      "Epoch 80/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.5775 - accuracy: 0.5084\n",
      "Epoch 81/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5592 - accuracy: 0.5556\n",
      "Epoch 82/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5732 - accuracy: 0.5455\n",
      "Epoch 83/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5503 - accuracy: 0.5455\n",
      "Epoch 84/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.5214 - accuracy: 0.5286\n",
      "Epoch 85/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.5029 - accuracy: 0.6027\n",
      "Epoch 86/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.4263 - accuracy: 0.5859\n",
      "Epoch 87/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.5193 - accuracy: 0.5320\n",
      "Epoch 88/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.4226 - accuracy: 0.5993\n",
      "Epoch 89/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.4779 - accuracy: 0.5589\n",
      "Epoch 90/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.4118 - accuracy: 0.5791\n",
      "Epoch 91/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.4370 - accuracy: 0.5926\n",
      "Epoch 92/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.3295 - accuracy: 0.6465\n",
      "Epoch 93/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.3805 - accuracy: 0.6498\n",
      "Epoch 94/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.3167 - accuracy: 0.6633\n",
      "Epoch 95/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.3113 - accuracy: 0.6162\n",
      "Epoch 96/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.3541 - accuracy: 0.5926\n",
      "Epoch 97/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.3294 - accuracy: 0.6094\n",
      "Epoch 98/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.3330 - accuracy: 0.6162\n",
      "Epoch 99/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2597 - accuracy: 0.6397\n",
      "Epoch 100/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2845 - accuracy: 0.6162\n",
      "Epoch 101/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2788 - accuracy: 0.6162\n",
      "Epoch 102/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.2929 - accuracy: 0.6263\n",
      "Epoch 103/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.3184 - accuracy: 0.6195\n",
      "Epoch 104/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2795 - accuracy: 0.6229\n",
      "Epoch 105/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.2381 - accuracy: 0.6431\n",
      "Epoch 106/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.2176 - accuracy: 0.6364\n",
      "Epoch 107/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2776 - accuracy: 0.6431\n",
      "Epoch 108/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.3376 - accuracy: 0.5859\n",
      "Epoch 109/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.3529 - accuracy: 0.6162\n",
      "Epoch 110/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2795 - accuracy: 0.6195\n",
      "Epoch 111/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.2313 - accuracy: 0.6061\n",
      "Epoch 112/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2165 - accuracy: 0.6397\n",
      "Epoch 113/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1977 - accuracy: 0.6465\n",
      "Epoch 114/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1783 - accuracy: 0.6566\n",
      "Epoch 115/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1679 - accuracy: 0.6801\n",
      "Epoch 116/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1973 - accuracy: 0.6498\n",
      "Epoch 117/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.1708 - accuracy: 0.6364\n",
      "Epoch 118/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.0844 - accuracy: 0.6970\n",
      "Epoch 119/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1836 - accuracy: 0.6330\n",
      "Epoch 120/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.1369 - accuracy: 0.6700\n",
      "Epoch 121/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 1.1251 - accuracy: 0.6734\n",
      "Epoch 122/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.1721 - accuracy: 0.6768\n",
      "Epoch 123/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.2229 - accuracy: 0.5791\n",
      "Epoch 124/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.1344 - accuracy: 0.6801\n",
      "Epoch 125/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1226 - accuracy: 0.6566\n",
      "Epoch 126/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1744 - accuracy: 0.6263\n",
      "Epoch 127/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0750 - accuracy: 0.6801\n",
      "Epoch 128/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1078 - accuracy: 0.6700\n",
      "Epoch 129/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1120 - accuracy: 0.6902\n",
      "Epoch 130/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0829 - accuracy: 0.6835\n",
      "Epoch 131/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0224 - accuracy: 0.6768\n",
      "Epoch 132/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0880 - accuracy: 0.6869\n",
      "Epoch 133/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.0707 - accuracy: 0.6566\n",
      "Epoch 134/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.0456 - accuracy: 0.6869\n",
      "Epoch 135/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0313 - accuracy: 0.6970\n",
      "Epoch 136/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0130 - accuracy: 0.6970\n",
      "Epoch 137/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9516 - accuracy: 0.7205\n",
      "Epoch 138/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9421 - accuracy: 0.7205\n",
      "Epoch 139/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9953 - accuracy: 0.7205\n",
      "Epoch 140/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0318 - accuracy: 0.6936\n",
      "Epoch 141/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9706 - accuracy: 0.6734\n",
      "Epoch 142/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9636 - accuracy: 0.7340\n",
      "Epoch 143/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9922 - accuracy: 0.6970\n",
      "Epoch 144/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9619 - accuracy: 0.7273\n",
      "Epoch 145/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9614 - accuracy: 0.6801\n",
      "Epoch 146/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9734 - accuracy: 0.7071\n",
      "Epoch 147/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9310 - accuracy: 0.7374\n",
      "Epoch 148/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9314 - accuracy: 0.7374\n",
      "Epoch 149/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.0006 - accuracy: 0.6902\n",
      "Epoch 150/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9219 - accuracy: 0.7172\n",
      "Epoch 151/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9417 - accuracy: 0.6902\n",
      "Epoch 152/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9242 - accuracy: 0.7138\n",
      "Epoch 153/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9166 - accuracy: 0.7441\n",
      "Epoch 154/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9586 - accuracy: 0.7306\n",
      "Epoch 155/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9141 - accuracy: 0.7508\n",
      "Epoch 156/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9232 - accuracy: 0.7205\n",
      "Epoch 157/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8573 - accuracy: 0.7576\n",
      "Epoch 158/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9096 - accuracy: 0.7037\n",
      "Epoch 159/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8487 - accuracy: 0.7576\n",
      "Epoch 160/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9246 - accuracy: 0.7037\n",
      "Epoch 161/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9323 - accuracy: 0.6936\n",
      "Epoch 162/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8805 - accuracy: 0.7205\n",
      "Epoch 163/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8718 - accuracy: 0.7138\n",
      "Epoch 164/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8854 - accuracy: 0.7374\n",
      "Epoch 165/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9037 - accuracy: 0.7172\n",
      "Epoch 166/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.9253 - accuracy: 0.7441\n",
      "Epoch 167/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8824 - accuracy: 0.7407\n",
      "Epoch 168/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8425 - accuracy: 0.7441\n",
      "Epoch 169/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8813 - accuracy: 0.7508\n",
      "Epoch 170/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.8642 - accuracy: 0.7306\n",
      "Epoch 171/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8573 - accuracy: 0.7205\n",
      "Epoch 172/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8456 - accuracy: 0.7273\n",
      "Epoch 173/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8536 - accuracy: 0.7407\n",
      "Epoch 174/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8059 - accuracy: 0.7576\n",
      "Epoch 175/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8394 - accuracy: 0.7340\n",
      "Epoch 176/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8601 - accuracy: 0.7374\n",
      "Epoch 177/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.7973 - accuracy: 0.7778\n",
      "Epoch 178/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8710 - accuracy: 0.6902\n",
      "Epoch 179/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8915 - accuracy: 0.7138\n",
      "Epoch 180/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9062 - accuracy: 0.6869\n",
      "Epoch 181/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8539 - accuracy: 0.7273\n",
      "Epoch 182/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8546 - accuracy: 0.7475\n",
      "Epoch 183/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8690 - accuracy: 0.7172\n",
      "Epoch 184/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8304 - accuracy: 0.7340\n",
      "Epoch 185/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8516 - accuracy: 0.7205\n",
      "Epoch 186/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9336 - accuracy: 0.7071\n",
      "Epoch 187/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8480 - accuracy: 0.7273\n",
      "Epoch 188/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8258 - accuracy: 0.7441\n",
      "Epoch 189/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8126 - accuracy: 0.7374\n",
      "Epoch 190/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.7634 - accuracy: 0.7677\n",
      "Epoch 191/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.7485 - accuracy: 0.7778\n",
      "Epoch 192/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8195 - accuracy: 0.7441\n",
      "Epoch 193/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.7793 - accuracy: 0.7677\n",
      "Epoch 194/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.7988 - accuracy: 0.7710\n",
      "Epoch 195/200\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.7983 - accuracy: 0.7475\n",
      "Epoch 196/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8468 - accuracy: 0.7441\n",
      "Epoch 197/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9265 - accuracy: 0.6768\n",
      "Epoch 198/200\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.8741 - accuracy: 0.7475\n",
      "Epoch 199/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8553 - accuracy: 0.7306\n",
      "Epoch 200/200\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.8191 - accuracy: 0.7475\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1227 - accuracy: 0.4800\n",
      "Test accuracy: 0.47999998927116394\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions from the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Find the most likely prediction for each sample\n",
    "most_likely_predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 15, 11,  3, 15,  5,  6, 20,  8,  9,  8, 11, 21, 13,  6,  1, 16,\n",
       "       17, 17, 19, 20,  3,  3, 18, 24], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = label_encoder.inverse_transform(most_likely_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_labels = train_data.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctamente predecido en:  eat food\n",
      "Correctamente predecido en:  fine\n",
      "Correctamente predecido en:  finish\n",
      "Correctamente predecido en:  hello\n",
      "Correctamente predecido en:  help\n",
      "Correctamente predecido en:  like\n",
      "Correctamente predecido en:  milk\n",
      "Correctamente predecido en:  no\n",
      "Correctamente predecido en:  please\n",
      "Correctamente predecido en:  see you later\n",
      "Correctamente predecido en:  sign\n",
      "Correctamente predecido en:  yes\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predicted_labels)):\n",
    "    if (predicted_labels[i] == expected_labels[i]):\n",
    "        print(\"Correctamente predecido en: \", predicted_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, LSTM, TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(max_seq_length,num_features)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = max_seq_length * 80\n",
    "# num_timesteps = max_seq_length * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train).float()\n",
    "# X_test = torch.from_numpy(X_test).float()\n",
    "# y_train = torch.from_numpy(y_train).float()\n",
    "# y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ASLModel(nn.Module):\n",
    "#     def __init__(self, input_size, output_size):\n",
    "#         super(ASLModel, self).__init__()\n",
    "#         # RuntimeError: mat1 and mat2 shapes cannot be multiplied (6912x84 and 108x2048)\n",
    "#         # (6912x84 and 83x2048)\n",
    "\n",
    "#         self.linear1 = nn.Linear(input_size, 2048)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(1024, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.linear1(x)\n",
    "#         # x = self.relu1(x)\n",
    "#         # x = self.linear2(x)\n",
    "#         # x = self.relu2(x)\n",
    "#         # x = self.linear3(x)\n",
    "#         # print(x.shape)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Initialization\n",
    "# model = ASLModel(input_size=X_train.shape[1], output_size=y_train.shape[1]).to(device)\n",
    "# # Optimization Setup\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Preparation\n",
    "# train_data = TensorDataset(X_train, y_train)\n",
    "# test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# # DataLoader\n",
    "# BATCH_SIZE = 16\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reverse the JSON dictionary\n",
    "# pred_to_char = {value: key for key, value in char_to_pred.items()}\n",
    "# def reverse_to_char(data):\n",
    "#     phrase = \"\"\n",
    "#     for i in data:\n",
    "#         phrase += str(pred_to_char.get(int(i.item())) if int(i.item()) in pred_to_char else \"_\")\n",
    "    \n",
    "#     return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RuntimeError: The size of tensor a (108) must match the size of tensor b (64) at non-singleton dimension \n",
    "# for inputs, targets in train_loader:\n",
    "#     print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 5\n",
    "# loss_fn = nn.SmoothL1Loss() \n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     train_correct = 0\n",
    "\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # labels = labels.long()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Calculate the loss\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         # Accumulate the loss\n",
    "#         train_loss += loss.item() * inputs.size(0)\n",
    "#         # Accumulate the total number of samples\n",
    "#         train_correct += inputs.size(0)\n",
    "\n",
    "#     train_loss = train_loss / train_correct\n",
    "#     train_accuracy = train_correct / len(train_data)\n",
    "\n",
    "#     # Evaluation\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     levenshtein_distance = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             # labels = labels.long()\n",
    "\n",
    "#             print(\"input\")\n",
    "#             print(inputs)\n",
    "#             outputs = model(inputs)     \n",
    "#             print(outputs)\n",
    "#             # Get the predicted labels\n",
    "#             _, predicted = torch.max(outputs.data, dim=1)\n",
    "#             # Calculate the loss\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             # Accumulate the loss\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             # Accumulate the total number of samples\n",
    "#             test_correct += inputs.size(0)\n",
    "\n",
    "#             print(predicted.size())\n",
    "#             outputs_array = outputs.detach().cpu().numpy()\n",
    "#             targets_array = labels.detach().cpu().numpy()\n",
    "#             # Convert predictions and targets to letter sequences\n",
    "#             pred_labels = [[pred_to_char[int(label)] for label in output if int(label) in pred_to_char ] if len(output) > 0 else [] for output in outputs_array.round()]\n",
    "#             target_labels = [[pred_to_char[label] for label in target if int(label) in pred_to_char  ] if len(target) > 0 else [] for target in targets_array.round()]\n",
    "\n",
    "#             print(\"Predicted: \", pred_labels)\n",
    "#             print(\"Expected: \", target_labels)\n",
    "#             break\n",
    "\n",
    "#             # # Calculate Levenshtein distance\n",
    "#             # levenshtein_distance += calculate_levenshtein_distance(pred_labels, target_labels)\n",
    "            \n",
    "#     # test_loss = test_loss / test_correct\n",
    "#     # test_accuracy = test_correct / len(test_data)\n",
    "#     # average_levenshtein_distance = levenshtein_distance / len(test_data)\n",
    "\n",
    "#     # Print epoch results\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "#     # print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
    "#     # print(f\"Average Levenshtein Distance: {average_levenshtein_distance:.4f}\")\n",
    "#     print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
