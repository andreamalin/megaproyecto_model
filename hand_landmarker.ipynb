{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import re\n",
    "from moviepy.editor import *\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "df = pd.DataFrame()\n",
    "frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust video duration, frame rate, width, and height\n",
    "def adjust_video_duration(video_path, output_duration, output_fps, width, height):\n",
    "    video_clip = VideoFileClip(video_path, target_resolution=(height, width))\n",
    "    video_clip = video_clip.without_audio()\n",
    "    \n",
    "    duration = video_clip.duration\n",
    "    # getting only first 5 seconds\n",
    "    clip = video_clip.subclip(0, duration)\n",
    "    \n",
    "    # applying speed effect\n",
    "    final = clip.fx(vfx.speedx, duration)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video(video, target, sequence_id, target_width, target_height):\n",
    "    global frames\n",
    "    added_rows = 0\n",
    "    # For webcam input:\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the current duration of the video\n",
    "    video_duration = frame_count / fps\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the BGR frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame with the hand tracking model\n",
    "        results = hands.process(rgb_frame)\n",
    "         \n",
    "        if results.multi_hand_landmarks:\n",
    "            row_data = {\n",
    "                \"sequence_id\": sequence_id,\n",
    "                \"target\": target\n",
    "            }\n",
    "            detected_hands = []\n",
    "            for idx, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Iterate through detected hand landmarks\n",
    "                for landmark_idx, landmark in enumerate(landmarks.landmark):\n",
    "                    x, y = landmark.x * frame.shape[1], landmark.y * frame.shape[0]\n",
    "                    \n",
    "                    # Determine hand side based on the index of the hand in the list\n",
    "                    if idx == 0:\n",
    "                        hand_side = \"Left\"\n",
    "                    else:\n",
    "                        hand_side = \"Right\"\n",
    "                    \n",
    "                    row_data[f'x_{hand_side}_hand_{landmark_idx}'] =  x / target_width\n",
    "                    row_data[f'y_{hand_side}_hand_{landmark_idx}'] =  y / target_height\n",
    "            \n",
    "                    detected_hands.append(hand_side)\n",
    "\n",
    "                    # Draw circles on the frame\n",
    "                    cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                    cv2.putText(frame, hand_side, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            added_rows += 1\n",
    "            frames.append(row_data)\n",
    "        cv2.imshow('Hand Tracking', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        elif (added_rows == 30 and fps > 30):\n",
    "            print('-' * 20)\n",
    "            print(\"FPS: \", fps, \" VIDEO DURATION: \", video_duration, \"TARGET: \", target)\n",
    "            print('-' * 20)\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34468\\1352803112.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mextract_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34468\\2937975184.py\u001b[0m in \u001b[0;36mextract_video\u001b[1;34m(video, target, sequence_id, target_width, target_height)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Process the frame with the hand tracking model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_hand_landmarks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    363\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m     \u001b[1;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# output stream names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import time\n",
    "os.chdir(\"D:\\Documents\\Trabajo\\megaproyecto_model\\data/asl_videos\")\n",
    "\n",
    "sequence_id = 0\n",
    "for file in glob.glob(\"*.mp4\"):\n",
    "    sequence_id += 1\n",
    "    name = \" \".join(file.split(\" \")[:-1])\n",
    "\n",
    "    input_path = f'D:\\Documents\\Trabajo\\megaproyecto_model\\data/asl_videos/{file}'\n",
    "    output_path = 'D:\\Documents\\Trabajo\\megaproyecto_model\\data/asl_preprocessing/adjusted_video.mp4'\n",
    "\n",
    "    target_width = 640\n",
    "    target_height = 360\n",
    "    desired_fps = 30\n",
    "\n",
    "    if (name == \"no\"):\n",
    "        adjusted_video = adjust_video_duration(\n",
    "            video_path=input_path,\n",
    "            output_duration=1,\n",
    "            output_fps=30,\n",
    "            width=target_width,\n",
    "            height=target_height\n",
    "        )\n",
    "        # Save the adjusted video to the output file\n",
    "        adjusted_video.write_videofile(output_path, codec='libx264', verbose=False, logger=None)\n",
    "\n",
    "\n",
    "        extract_video(output_path, name, sequence_id, target_width, target_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sequence_id'] = df['sequence_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"D:\\Documents\\Trabajo\\megaproyecto_model\" + \"/data.csv\"\n",
    "df.to_csv(dir_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"D:\\Documents\\Trabajo\\megaproyecto_model\" + \"/validation.csv\"\n",
    "\n",
    "import random\n",
    "grouped = df.groupby('target')\n",
    "selected_validation_ids = []\n",
    "for group_name, group_indices in grouped.groups.items():\n",
    "    ids = list(set(df.loc[group_indices]['sequence_id'].values))\n",
    "    if (len(ids) < 0):\n",
    "        raise (\"Error\")\n",
    "    selected_validation_ids.append(random.choice(ids))\n",
    "\n",
    "validation_df = df[df['sequence_id'].isin(selected_validation_ids)][[\"sequence_id\", \"target\"]]\n",
    "validation_df.to_csv(validation_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\Documents\\Trabajo\\megaproyecto_model\" + \"/train.csv\"\n",
    "train_df = df[~df['sequence_id'].isin(selected_validation_ids)][[\"sequence_id\", \"target\"]]\n",
    "train_df.to_csv(train_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
